<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/groundiff.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/GrounDiff/" target="_blank">GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</a>
    </strong>
    </papertitle>
    <br>
    <strong>Oussema Dhaouadi</strong>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>,
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>WACV</strong> 2026</em>
    <br>
    <a href="https://deepscenario.github.io/GrounDiff/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2511.10391" target="_blank">Paper</a>

<br>
We introduce GrounDiff, a diffusion-based method that generates DTMs from DSMs by iteratively removing non-ground structures, achieving up to 93% lower error than prior learning-based approaches, and present PrioStitch and GrounDiff+ for scalable high-resolution terrain generation and smooth road reconstruction with up to 81% lower distance error.
</td>
</table>


<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/ortholoc.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/OrthoLoC/" target="_blank">OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata</a>
    </strong>
    </papertitle>
    <br>
    <strong>Oussema Dhaouadi</strong>,
    <a href="https://ricma.netlify.app/" target="_blank">Riccardo Marin</a>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>,
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>NeurIPS</strong> 2025</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(Oral)</span>
    <br>
    <a href="https://deepscenario.github.io/OrthoLoC/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2509.18350" target="_blank">Paper</a> |
    <a href="https://cvg.cit.tum.de/webshare/g/papers/Dhaouadi/OrthoLoC/" target="_blank">Data</a> |
    <a href="https://github.com/deepscenario/OrthoLoC/" target="_blank">Github Repo</a>

<br>
We introduce OrthoLoC, the first large-scale UAV localization dataset with 16,425 images from Germany and the U.S., paired with orthophotos and digital surface models (DSMs) to enable benchmarking under domain shifts, and propose AdHoP, a refinement method that improves feature matching accuracy by up to 95% and reduces translation error by 63%, advancing lightweight high-precision aerial localization without GNSS or large 3D models.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/coprou.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://jchao-xie.github.io/CoProU/" target="_blank">CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://www.linkedin.com/in/jingchao-xie-16b724297/" target="_blank">Jingchao Xie*</a>,
    <strong>Oussema Dhaouadi*</strong>,
    <a href="https://wrchen530.github.io/" target="_blank">Weirong Chen</a>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>,
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>IV</strong> 2025</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(Oral - Best Paper Award)</span>
    <br>
    <a href="https://jchao-xie.github.io/CoProU/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2508.00568" target="_blank">Paper</a> |
    <a href="https://github.com/Jchao-Xie/CoProU" target="_blank">Github Repo</a> |
    <a href="https://deepscenario.github.io/FlexRoad/static/Poster_FlexRoad.pdf" target="_blank">Poster</a>

<br>
We propose CoProU-VO, an unsupervised visual odometry framework that propagates and combines uncertainty across temporal frames to robustly filter dynamic objects, leveraging vision transformers to jointly learn depth, uncertainty, and poses, and achieving superior performance on KITTI and nuScenes, especially in challenging dynamic and highway scenarios.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/flexroad.gif" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/FlexRoad/" target="_blank">Shape Your Ground: Refining Road Surfaces Beyond Planar Representations</a>
    </strong>
    </papertitle>
    <br>
    <strong>Oussema Dhaouadi</strong>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>IV</strong> 2025</em>
    <br>
    <a href="https://deepscenario.github.io/FlexRoad/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2504.16103" target="_blank">Paper</a> |
    <a href="https://cvg.cit.tum.de/webshare/g/papers/Dhaouadi/dhaouadi2025flexroad/" target="_blank">Data</a> |
    <a href="https://deepscenario.github.io/FlexRoad/static/Poster_FlexRoad.pdf" target="_blank">Poster</a>

<br>
We present FlexRoad, the first framework for smooth and accurate road surface reconstruction using NURBS fitting with Elevation-Constrained Spatial Road Clustering, and introduce the GeoRoad Dataset (GeRoD) for benchmarking, demonstrating that FlexRoad outperforms existing methods across diverse datasets and terrains while robustly handling noise and input variability.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/dsc3d.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/DSC3D/" target="_blank">Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</a>
    </strong>
    </papertitle>
    <br>
    <strong>Oussema Dhaouadi*</strong>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier*</a>, 
    <a href="https://www.linkedin.com/in/louis-inchingolo-5a8ba3179/" target="_blank">Louis Inchingolo*</a>, 
    <a href="https://www.linkedin.com/in/luca-wahl/" target="_blank">Luca Wahl</a>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://www.linkedin.com/in/luca-scalerandi/" target="_blank">Luca Scalerandi</a>,
    <a href="https://www.linkedin.com/in/nickwandelburg/" target="_blank">Nick Wandelburg</a>,
    <a href="https://www.linkedin.com/in/zhuolun-zhou/" target="_blank">Zhuolun Zhou</a>,
    <a href="https://www.linkedin.com/in/nijanthan-b-064765137/" target="_blank">Nijanthan Berinpanathan</a>,
    <a href="https://www.linkedin.com/in/holger-banzhaf-25226782/" target="_blank">Holger Banzhaf</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>IV</strong> 2025</em>
    <br>
    <a href="https://deepscenario.github.io/DSC3D/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2504.17371" target="_blank">Paper</a> |
    <a href="https://app.deepscenario.com/" target="_blank">Data</a> |
    <a href="https://deepscenario.github.io/DSC3D/static/Poster_DSC3D.pdf" target="_blank">Poster</a>

<br>
We introduce DSC3D, a large-scale, occlusion-free 3D trajectory dataset captured via a monocular drone tracking pipeline, featuring over 175,000 trajectories of 14 traffic participant types across diverse urban and suburban scenarios, designed to enhance autonomous driving systems with detailed environmental representations and improved obstacle interaction modeling.
</td>
</table>


<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/mono_ct.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://arxiv.org/pdf/2503.13743" target="_blank">MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent Teacher Models</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier*</a>, 
    <a href="https://www.linkedin.com/in/louis-inchingolo-5a8ba3179/" target="_blank">Louis Inchingolo*</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://yan-xia.github.io/" target="_blank">Yan Xia*</a>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>ICRA</strong> 2025</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(Oral)</span>
    <br>
    <a href="https://arxiv.org/pdf/2503.13743" target="_blank">Paper</a>
    <br>
    We address monocular 3D object detection across diverse sensors and environments with MonoCT, an unsupervised domain adaptation method that uses Generalized Depth Enhancement, Pseudo Label Scoring, and Diversity Maximization to generate high-quality pseudo labels, achieving up to 21% improvement over prior state-of-the-art methods across six benchmarks.
</td>
</table>


<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/trafficloc.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://tum-luk.github.io/projects/trafficloc/" target="_blank">TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://yan-xia.github.io/" target="_blank">Yan Xia*</a>, 
    <a href="https://tum-luk.github.io/" target="_blank">Yunxiang Lu*</a>, 
    <a href="https://rruisong.github.io/" target="_blank">Rui Song</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank">Jo√£o F. Henriques</a>, 
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>ICCV</strong> 2025</em>
    <br>
    <a href="https://tum-luk.github.io/projects/trafficloc/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2412.10308" target="_blank">Paper</a>
    <br>
    We tackle traffic camera localization in 3D maps by introducing TrafficLoc, a coarse-to-fine image-to-point cloud registration method with Geometry-guided Attention Loss, Inter-intra Contrastive Learning, and Dense Training Alignment, alongside the Carla Intersection dataset, achieving up to 86% improvement over prior methods and new state-of-the-art performance on KITTI and NuScenes for both in-vehicle and traffic cameras.
</td>
</table>



<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/cdrone.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/CDrone/" target="_blank">CARLA Drone: Monocular 3D Object Detection from a Different Perspective</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>, 
    <a href="https://www.linkedin.com/in/luca-scalerandi/" target="_blank">Luca Scalerandi</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>, 
    <a href="https://arnike.github.io/" target="_blank">Nikita Araslanov</a>, 
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br>
    <em><strong>GCPR</strong> 2024</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(Oral)</span>
    <br>
     <a href="https://deepscenario.github.io/CDrone/" target="_blank">Project Page</a> | 
     <a href="https://github.com/deepscenario/EvalMono3D" target="_blank">Github Repo</a> | 
    <a href="https://www.arxiv.org/pdf/2408.11958" target="_blank">Paper</a> | 
    <a href="https://cvg.cit.tum.de/webshare/g/cdrone/data/" target="_blank">Data</a>
    <br>
    We introduce the CARLA Drone (CDrone) dataset to expand 3D detection benchmarks with diverse camera perspectives and propose GroundMix, a ground-based 3D-consistent augmentation pipeline, demonstrating that prior methods struggle across these views while our approach significantly improves detection accuracy across both synthetic and real-world datasets.
  </td>
</table>



<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/capspose.png" width="100%"> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://www.dfki.uni-kl.de/~pagani/papers/Firintepe2021_ISMAR.pdf" target="_blank">Comparing Head and AR Glasses Pose Estimation</a>
    </strong>
    </papertitle>
    <br>
    <a href="https://av.dfki.de/members/firintepe/" target="_blank">Ahmet Firintepe</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://av.dfki.de/members/pagani/" target="_blank">Alain Pagani</a>, 
    <a href="https://av.dfki.de/members/stricker/" target="_blank">Didier Stricker</a>
    <br>
    <em><strong>ISMAR-Adjunct</strong> 2021</em>
    <br>
    <a href="https://www.dfki.uni-kl.de/~pagani/papers/Firintepe2021_ISMAR.pdf" target="_blank">Paper</a> | 
    <a href="https://ieeexplore.ieee.org/document/9585816/" target="_blank">IEEE</a>
    <br>
    We introduce CapsPose, a novel capsule-network-based 6-DoF pose estimator, and compare head pose versus AR glasses pose estimation, showing that AR glasses pose is generally more accurate, while CapsPose significantly outperforms existing methods on the HMDPose dataset.
  </td>
</table>
