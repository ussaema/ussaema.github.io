

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>About me - Oussema Dhaouadi</title>



<meta name="description" content="Ph.D. Candidate @TUM  Computer Vision Scientist @DeepScenario">







  <link rel="canonical" href="http://localhost:4000/">





  

  






  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Oussema Dhaouadi",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->

<!-- Open Graph protocol data (https://ogp.me/), used by social media -->
<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Oussema Dhaouadi"> 
<meta property="og:title" content="About me">



  <meta property="og:url" content="http://localhost:4000/">





<!-- end Open Graph protocol -->

<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Oussema Dhaouadi Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<!-- Support for Academicons -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<!-- favicon from https://commons.wikimedia.org/wiki/File:OOjs_UI_icon_academic-progressive.svg -->
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png"/>
<link rel="icon" type="image/svg+xml" href="http://localhost:4000/images/favicon.svg"/>
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png" sizes="32x32"/>
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-192x192.png" sizes="192x192"/>
<link rel="manifest" href="http://localhost:4000/images/manifest.json"/>
<link rel="icon" href="/images/favicon.ico"/>
<meta name="theme-color" content="#ffffff"/>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg persist"><a href="http://localhost:4000/">Oussema Dhaouadi</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000">About me</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/#publications">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/#other-projects">Other Projects</a></li>
          
          <li id="theme-toggle" class="masthead__menu-item persist tail">
            <a role="button" aria-labelledby="theme-icon"><i id="theme-icon" class="fa-solid fa-sun" aria-hidden="true" title="toggle theme"></i></a>
          </li>
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/profile.png" class="author__avatar" alt="Oussema Dhaouadi"  fetchpriority="high" />
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Oussema Dhaouadi</h3>
    
    <p class="author__bio">Ph.D. Candidate @TUM <br> Computer Vision Scientist @DeepScenario</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      <!-- Font Awesome icons / Biographic information  -->
      
        <li class="author__desktop"><i class="fas fa-fw fa-location-dot icon-pad-right" aria-hidden="true"></i>Munich, Germany</li>
      
      
      
      
        <li><a href="mailto:oussema.dhaouadi@tum.de"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>Email</a></li>
      

      <!-- Font Awesome and Academicons icons / Academic websites -->
      
            
      
        <li><a href="https://scholar.google.de/citations?user=NMvQtL0AAAAJ&hl=de&oi=sra"><i class="ai ai-google-scholar ai-fw icon-pad-right"></i>Google Scholar</a></li>
      
      
      
      
        <li><a href="https://orcid.org/0009-0008-6842-5220"><i class="ai ai-orcid ai-fw icon-pad-right"></i>ORCID</a></li>
      
                              
      
      
      

      <!-- Font Awesome icons / Repositories and software development -->
      
            
            
      
        <li><a href="https://github.com/ussaema"><i class="fab fa-fw fa-github icon-pad-right" aria-hidden="true"></i>GitHub</a></li>
      
            
            

      <!-- Font Awesome icons / Social media -->
              
      
        <li><a href="https://bsky.app/profile/ussaema.bsky.social"><i class="fab fa-fw fa-bluesky icon-pad-right" aria-hidden="true"></i>Bluesky</a></li>
      
      
            
      
                  
                  
      
            
            
      
        <li><a href="https://www.linkedin.com/in/oussema-dhaouadi"><i class="fab fa-fw fa-linkedin icon-pad-right" aria-hidden="true"></i>LinkedIn</a></li>
            
      
            
                  
            
      
            
            
      
              
      
                      
      
      
            
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="About me">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">About me
</h1>
          
        
        
            
        </header>
      

      <section class="page__content" itemprop="text">
        <head>
 
  <style>
    @media (max-width: 768px) {  /* when screen width ≤ 768px (typical mobile) */
      table.responsive td {
        display: block;
        width: 100% !important;  /* stack each cell full width */
      }
      table.responsive tr {
        display: block;
      }
    }
  </style>
</head>
<hr />

<p>Hi, I’m Oussema Dhaouadi, a PhD student at the <a href="https://www.tum.de/" target="_blank">Technical University of Munich</a> in the Computer Vision Group led by <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Prof. Daniel Cremers</a>. I work in collaboration with <a href="https://deepscenario.com" target="_blank">DeepScenario</a> on 3D reconstruction and localization.</p>

<p>Before starting my PhD, I was a Machine Learning Engineer at <a href="https://knowlix.ai/" target="_blank">Knowlix GmbH</a>, where I built automated solutions for paperwork. I received my Master’s degree with high distinction in Electrical Engineering and Information Technology from TUM. I completed my thesis at the <a href="https://www.bmwgroup.com/" target="_blank">BMW Group Research and Technology Center</a> under the supervision of <a href="https://av.dfki.de/members/firintepe/" target="_blank">Dr. Ahmet Firintepe</a> and  <a href="https://www.ce.cit.tum.de/lmt/team/mitarbeiter/steinbach-eckehard/" target="_blank">Prof. Eckehard Steinbach</a> and spent a semester abroad at <a href="https://www.ntu.edu.sg/" target="_blank">NTU Singapore</a>. I also hold a Bachelor’s degree in Electrical Engineering and Information Technology from TUM, where I completed my Bachelor’s thesis under the supervision of <a href="https://www.linkedin.com/in/alexander-kuhn-4a680714b/" target="_blank">Alexander Kuhn</a> and <a href="https://www.ce.cit.tum.de/en/air/people/prof-dr-ing-habil-alois-knoll/" target="_blank">Prof. Alois Knoll</a>. During my studies, I gained additional experience at <a href="https://global.fujitsu/" target="_blank">Fujitsu</a> and <a href="https://www.rohde-schwarz.com/" target="_blank">Rohde &amp; Schwarz</a>.</p>

<div class="institution-list">

<a href="https://www.tum.de/en/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/tum.png" />
</a>


<a href="https://deepscenario.com/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/deepscenario.png" />
</a>


<a href="https://mcml.ai/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/mcml.png" />
</a>

<a href="https://knowlix.ai/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/knowlix.png" />
</a>

</div>

<div class="institution-list">



<a href="https://www.bmwgroup.com/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/bmw.png" />
</a>


<a href="https://www.ntu.edu.sg/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/ntu.png" />
</a>

<a href="https://www.rohde-schwarz.com/" target="_blank">
    <img src="./images/logos/rs.png" />
</a>


<a href="https://global.fujitsu/" target="_blank" style="text-decoration: none;">
    <img src="./images/logos/fujitsu.png" />
</a>


</div>

<h2 id="news">News</h2>
<div id="news-container" style="overflow-y: auto; padding-right: 10px;">
  <ul id="news-list">
    <li><em>2025.09</em>: <a href="https://deepscenario.github.io/OrthoLoC/" target="_blank">OrthoLoC</a> accepted as an <b>oral presentation</b> at <b>NeurIPS 2025</b>.</li>
    <li><em>2025.07</em>: <a href="https://jchao-xie.github.io/CoProU/" target="_blank">CoProU-VO</a> accepted as an <b>oral presentation</b> at <b>GCPR 2025</b>.</li>
    <li><em>2025.06</em>: <a href="https://tum-luk.github.io/projects/trafficloc/" target="_blank">TrafficLoc</a> accepted at <b>ICCV 2025</b>.</li>
    <li><em>2025.03</em>: Two papers, <a href="https://deepscenario.github.io/DSC3D/" target="_blank">DSC3D</a> and <a href="https://deepscenario.github.io/FlexRoad/" target="_blank">FlexRoad</a>, accepted at <b>IV 2025</b>.</li> 
    <li><em>2025.01</em>: <a href="https://arxiv.org/pdf/2503.13743" target="_blank">MonoCT</a> accepted at <b>ICRA 2025</b>.</li> 
    <li><em>2024.08</em>: <a href="https://deepscenario.github.io/CDrone/" target="_blank">CDrone</a> accepted at <b>GCPR 2024</b>.</li>
    <li><em>2023.12</em>: Started PhD at <a href="https://cvg.cit.tum.de/" target="_blank">TUM Computer Vision Group</a> under <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Prof. Daniel Cremers</a>.</li>
    <li><em>2021.11</em>: Completed the <b>TUM Skills Excellence</b> program.</li>
    <li><em>2021.09</em>: Joined <b>Knowlix GmbH</b> as Machine Learning Engineer.</li>
    <li><em>2021.07</em>: Earned M.Sc. in <b>Electrical Engineering and Information Technology</b> from <b>TUM</b>.</li>
    <li><em>2021.03</em>: Internship at BMW Group; co-authored <a href="https://www.dfki.uni-kl.de/~pagani/papers/Firintepe2021_ISMAR.pdf" target="_blank">CapsPose</a>, accepted at <b>ISMAR-Adjunct 2021</b>.</li>
    <li><em>2020.09</em>: Started research internship at BMW Group.</li>
    <li><em>2020.01–2020.05</em>: Semester abroad at <b>NTU Singapore</b>.</li>
    <li><em>2018.07</em>: Started as Software Developer at <b>Rohde &amp; Schwarz GmbH</b> after leaving <b>Fujitsu</b>.</li>
    <li><em>2017.03</em>: Began role as Hardware Developer at <b>Fujitsu</b>.</li>
    <li><em>2015.10</em>: Started B.Sc. in <b>Electrical Engineering and Information Technology</b> at <b>TUM</b>.</li>
  </ul>
</div>

<h2 id="publications">Publications</h2>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/ortholoc.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/OrthoLoC/" target="_blank">OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata</a>
    </strong>
    </papertitle>
    <br />
    <strong>Oussema Dhaouadi</strong>,
    <a href="https://ricma.netlify.app/" target="_blank">Riccardo Marin</a>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>,
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>NeurIPS</strong> 2025</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(oral)</span>
    <br />
    <a href="https://deepscenario.github.io/OrthoLoC/" target="_blank">Project Page</a> | 
    Paper (coming soon) |
    <a href="https://cvg.cit.tum.de/webshare/g/papers/Dhaouadi/OrthoLoC/" target="_blank">Data</a> |
    <a href="https://github.com/deepscenario/OrthoLoC/" target="_blank">Github Repo</a>

<br />
We introduce OrthoLoC, the first large-scale UAV localization dataset with 16,425 images from Germany and the U.S., paired with orthophotos and digital surface models (DSMs) to enable benchmarking under domain shifts, and propose AdHoP, a refinement method that improves feature matching accuracy by up to 95% and reduces translation error by 63%, advancing lightweight high-precision aerial localization without GNSS or large 3D models.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/coprou.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://jchao-xie.github.io/CoProU/" target="_blank">CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry</a>
    </strong>
    </papertitle>
    <br />
    <a href="https://www.linkedin.com/in/jingchao-xie-16b724297/" target="_blank">Jingchao Xie*</a>,
    <strong>Oussema Dhaouadi*</strong>,
    <a href="https://wrchen530.github.io/" target="_blank">Weirong Chen</a>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>,
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>IV</strong> 2025</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(oral)</span>
    <br />
    <a href="https://jchao-xie.github.io/CoProU/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2508.00568" target="_blank">Paper</a> |
    <a href="https://github.com/Jchao-Xie/CoProU" target="_blank">Github Repo</a> |
    <a href="https://deepscenario.github.io/FlexRoad/static/Poster_FlexRoad.pdf" target="_blank">Poster</a>

<br />
We propose CoProU-VO, an unsupervised visual odometry framework that propagates and combines uncertainty across temporal frames to robustly filter dynamic objects, leveraging vision transformers to jointly learn depth, uncertainty, and poses, and achieving superior performance on KITTI and nuScenes, especially in challenging dynamic and highway scenarios.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/flexroad.gif" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/FlexRoad/" target="_blank">Shape Your Ground: Refining Road Surfaces Beyond Planar Representations</a>
    </strong>
    </papertitle>
    <br />
    <strong>Oussema Dhaouadi</strong>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>IV</strong> 2025</em>
    <br />
    <a href="https://deepscenario.github.io/FlexRoad/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2504.16103" target="_blank">Paper</a> |
    <a href="https://cvg.cit.tum.de/webshare/g/papers/Dhaouadi/dhaouadi2025flexroad/" target="_blank">Data</a> |
    <a href="https://deepscenario.github.io/FlexRoad/static/Poster_FlexRoad.pdf" target="_blank">Poster</a>

<br />
We present FlexRoad, the first framework for smooth and accurate road surface reconstruction using NURBS fitting with Elevation-Constrained Spatial Road Clustering, and introduce the GeoRoad Dataset (GeRoD) for benchmarking, demonstrating that FlexRoad outperforms existing methods across diverse datasets and terrains while robustly handling noise and input variability.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/dsc3d.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/DSC3D/" target="_blank">Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</a>
    </strong>
    </papertitle>
    <br />
    <strong>Oussema Dhaouadi*</strong>,
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier*</a>, 
    <a href="https://www.linkedin.com/in/louis-inchingolo-5a8ba3179/" target="_blank">Louis Inchingolo*</a>, 
    <a href="https://www.linkedin.com/in/luca-wahl/" target="_blank">Luca Wahl</a>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://www.linkedin.com/in/luca-scalerandi/" target="_blank">Luca Scalerandi</a>,
    <a href="https://www.linkedin.com/in/nickwandelburg/" target="_blank">Nick Wandelburg</a>,
    <a href="https://www.linkedin.com/in/zhuolun-zhou/" target="_blank">Zhuolun Zhou</a>,
    <a href="https://www.linkedin.com/in/nijanthan-b-064765137/" target="_blank">Nijanthan Berinpanathan</a>,
    <a href="https://www.linkedin.com/in/holger-banzhaf-25226782/" target="_blank">Holger Banzhaf</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>IV</strong> 2025</em>
    <br />
    <a href="https://deepscenario.github.io/DSC3D/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2504.17371" target="_blank">Paper</a> |
    <a href="https://app.deepscenario.com/" target="_blank">Data</a> |
    <a href="https://deepscenario.github.io/DSC3D/static/Poster_DSC3D.pdf" target="_blank">Poster</a>

<br />
We introduce DSC3D, a large-scale, occlusion-free 3D trajectory dataset captured via a monocular drone tracking pipeline, featuring over 175,000 trajectories of 14 traffic participant types across diverse urban and suburban scenarios, designed to enhance autonomous driving systems with detailed environmental representations and improved obstacle interaction modeling.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/mono_ct.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://arxiv.org/pdf/2503.13743" target="_blank">MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent Teacher Models</a>
    </strong>
    </papertitle>
    <br />
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier*</a>, 
    <a href="https://www.linkedin.com/in/louis-inchingolo-5a8ba3179/" target="_blank">Louis Inchingolo*</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://yan-xia.github.io/" target="_blank">Yan Xia*</a>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>,
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>ICRA</strong> 2025</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(oral)</span>
    <br />
    <a href="https://arxiv.org/pdf/2503.13743" target="_blank">Paper</a>
    <br />
    We address monocular 3D object detection across diverse sensors and environments with MonoCT, an unsupervised domain adaptation method that uses Generalized Depth Enhancement, Pseudo Label Scoring, and Diversity Maximization to generate high-quality pseudo labels, achieving up to 21% improvement over prior state-of-the-art methods across six benchmarks.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/trafficloc.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://tum-luk.github.io/projects/trafficloc/" target="_blank">TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes</a>
    </strong>
    </papertitle>
    <br />
    <a href="https://yan-xia.github.io/" target="_blank">Yan Xia*</a>, 
    <a href="https://tum-luk.github.io/" target="_blank">Yunxiang Lu*</a>, 
    <a href="https://rruisong.github.io/" target="_blank">Rui Song</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank">João F. Henriques</a>, 
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>ICCV</strong> 2025</em>
    <br />
    <a href="https://tum-luk.github.io/projects/trafficloc/" target="_blank">Project Page</a> | 
    <a href="https://arxiv.org/pdf/2412.10308" target="_blank">Paper</a>
    <br />
    We tackle traffic camera localization in 3D maps by introducing TrafficLoc, a coarse-to-fine image-to-point cloud registration method with Geometry-guided Attention Loss, Inter-intra Contrastive Learning, and Dense Training Alignment, alongside the Carla Intersection dataset, achieving up to 86% improvement over prior methods and new state-of-the-art performance on KITTI and NuScenes for both in-vehicle and traffic cameras.
</td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/cdrone.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://deepscenario.github.io/CDrone/" target="_blank">CARLA Drone: Monocular 3D Object Detection from a Different Perspective</a>
    </strong>
    </papertitle>
    <br />
    <a href="https://cvg.cit.tum.de/members/mejo" target="_blank">Johannes Meier</a>, 
    <a href="https://www.linkedin.com/in/luca-scalerandi/" target="_blank">Luca Scalerandi</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://jacqueskaiser.com/" target="_blank">Jacques Kaiser</a>, 
    <a href="https://arnike.github.io/" target="_blank">Nikita Araslanov</a>, 
    <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a>
    <br />
    <em><strong>GCPR</strong> 2024</em> <span style="color:rgb(255, 50, 0);font-weight: bold;">(oral)</span>
    <br />
     <a href="https://deepscenario.github.io/CDrone/" target="_blank">Project Page</a> | 
     <a href="https://github.com/deepscenario/EvalMono3D" target="_blank">Github Repo</a> | 
    <a href="https://www.arxiv.org/pdf/2408.11958" target="_blank">Paper</a> | 
    <a href="https://cvg.cit.tum.de/webshare/g/cdrone/data/" target="_blank">Data</a>
    <br />
    We introduce the CARLA Drone (CDrone) dataset to expand 3D detection benchmarks with diverse camera perspectives and propose GroundMix, a ground-based 3D-consistent augmentation pipeline, demonstrating that prior methods struggle across these views while our approach significantly improves detection accuracy across both synthetic and real-world datasets.
  </td>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <td width="40%">
    <div class="one">
    <img src="/images/publications/capspose.png" width="100%" /> </div>
  </td>
  <td valign="top" width="75%">
    <papertitle>
    <strong>
      <a href="https://www.dfki.uni-kl.de/~pagani/papers/Firintepe2021_ISMAR.pdf" target="_blank">Comparing Head and AR Glasses Pose Estimation</a>
    </strong>
    </papertitle>
    <br />
    <a href="https://av.dfki.de/members/firintepe/" target="_blank">Ahmet Firintepe</a>, 
    <strong>Oussema Dhaouadi</strong>, 
    <a href="https://av.dfki.de/members/pagani/" target="_blank">Alain Pagani</a>, 
    <a href="https://av.dfki.de/members/stricker/" target="_blank">Didier Stricker</a>
    <br />
    <em><strong>ISMAR-Adjunct</strong> 2021</em>
    <br />
    <a href="https://www.dfki.uni-kl.de/~pagani/papers/Firintepe2021_ISMAR.pdf" target="_blank">Paper</a> | 
    <a href="https://ieeexplore.ieee.org/document/9585816/" target="_blank">IEEE</a>
    <br />
    We introduce CapsPose, a novel capsule-network-based 6-DoF pose estimator, and compare head pose versus AR glasses pose estimation, showing that AR glasses pose is generally more accurate, while CapsPose significantly outperforms existing methods on the HMDPose dataset.
  </td>
</table>

<h2 id="other-projects">Other Projects</h2>
<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/seqcapsgan.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/SeqCapsGAN" target="_blank">
            SeqCapsGAN: Generating Image Stylized Captioning using Capsule GANs
          </a>
        </strong>
      </papertitle>
      <em>Course: Natural Language Processing</em>
<br />
    <em>2020</em>
      <br />
      <a href="https://github.com/ussaema/SeqCapsGAN" target="_blank">Github Repo</a> | 
      <a href="https://github.com/ussaema/SeqCapsGAN/blob/master/Paper.pdf" target="_blank">Report</a>
      <br />
We introduce SeqCapsGAN, a stylized image captioning framework that combines Generative Adversarial Networks and Capsule Networks to generate human-like captions with sentiment, outperforming baseline models on key evaluation metrics.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/emg_gan.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/EMGCaps" target="_blank">
            EMGCaps: Electromyographic Hand Gesture Recognition using Capsule Network
          </a>
        </strong>
      </papertitle>
      <em>Practical course at NTU</em>
<br />
    <em>2020</em>
      <br />
      <a href="https://github.com/ussaema/EMGCaps" target="_blank">Github Repo</a> | 
      <a href="https://github.com/ussaema/EMGCaps/blob/master/Paper.pdf" target="_blank">Report</a>
      <br />
We introduce EMG-Caps, a framework for hand gesture classification using Capsule Networks on sEMG signals from the NinaPro DB5 dataset, demonstrating that CapsNets outperform conventional classifiers and other deep learning methods—achieving up to 93.27% accuracy across 53 hand movements.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/robot_arm_control.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/Trajectory_Planning_using_Bi-RRT_Algorithm" target="_blank">
            Electromyography-based Control of a Simulated Robot Arm
          </a>
        </strong>
      </papertitle>
      <em>Practical course Networked and Cooperative Systems by <a href="https://www.ce.cit.tum.de/itr/hirche/" target="_blank">Prof. Sandra Hirche</a></em>
<br />
    <em>2019</em>
      <br />
      <a href="https://github.com/ussaema/Trajectory_Planning_using_Bi-RRT_Algorithm" target="_blank">Github Repo</a> |
      <a href="https://github.com/ussaema/Trajectory_Planning_using_Bi-RRT_Algorithm/blob/main/Paper.pdf" target="_blank">Report</a>
      <br />
We performed data acquisition and preprocessing, designed and evaluated classification systems using both conventional and neural network–based pattern recognition methods in TensorFlow and Keras, and implemented a real-time operating 6-DoF robot arm.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/robocup.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/Trajectory_Planning_using_Bi-RRT_Algorithm" target="_blank">
            Trajectory Planning for Humanoid RoboCup using bi-RTT Algorithm
          </a>
        </strong>
      </papertitle>
      <em>Practical course Advanced Lab Humanoid RoboCup by <a href="https://www.ce.cit.tum.de/ics/people/cheng/" target="_blank">Prof. Gordon Cheng</a></em>
<br />
    <em>2019</em>
      <br />
      <a href="https://github.com/ussaema/Trajectory_Planning_using_Bi-RRT_Algorithm" target="_blank">Github Repo</a>
      <br />
We implemented trajectory planning algorithms (RRT/RRT*) in C++, simulated and evaluated their performance in both simple and challenging scenarios, and conducted real-world testing on the NAO robot using ROS.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/oar.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/Hand_Gesture_Controlled_Obstacle_Avoiding_Robot" target="_blank">
            Hand Gesture Controlled Obstacle Avoiding Robot
          </a>
        </strong>
      </papertitle>
      <em>Project of the course Humanoid Sensors and Actuators by <a href="https://www.ce.cit.tum.de/ics/people/cheng/" target="_blank">Prof. Gordon Cheng</a></em>
<br />
    <em>2019</em>
      <br />
      <a href="https://github.com/ussaema/Hand_Gesture_Controlled_Obstacle_Avoiding_Robot" target="_blank">Github Repo</a>
      <br />
We developed Hand Gesture Controlled Obstacle Avoiding Robot, implementing embedded programming in C++ on an Atmega328P microcontroller, sensor signal processing with a 6DoF IMU, ultrasonic and IR sensors, actuation control for servos, gear motors, and vibration motors, and a custom communication protocol for a radio transmitter/receiver module.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/box_office.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/Box_Office_Sales_Estimation" target="_blank">
            Prediction Opening-Weekend Box Office Sales based on Movie Trailers
          </a>
        </strong>
      </papertitle>
      <em>Project of the course Applied Machine Learning by <a href="https://www.professoren.tum.de/diepold-klaus" target="_blank">Prof. Klaus Diepold</a></em>
<br />
    <em>2019</em>
    <br />
      <br />
      <a href="https://github.com/ussaema/Box_Office_Sales_Estimation" target="_blank">Github Repo</a>
      <br />
In this project, we presented a system for predicting opening weekend box office sales from movie trailers by combining metadata (e.g., actors, studio, genre) with audio-visual features (e.g., spectrograms, colors, emotions), and implemented a frontend that enables users to run predictions on arbitrary YouTube trailers.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/capsgan.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/Vector_Matrix_CapsuleGAN" target="_blank">
            Generative Modelling using Capsule Generative Adversarial Networks
          </a>
        </strong>
      </papertitle>
      <em>Bachelor Thesis supervised by <a href="https://www.linkedin.com/in/alexander-kuhn-4a680714b/" target="_blank">Alexander Kuhn</a>, <a href="https://www.ce.cit.tum.de/en/air/people/prof-dr-ing-habil-alois-knoll/" target="_blank">Prof. Alois Knoll</a>, and <a href="https://www.professoren.tum.de/diepold-klaus" target="_blank">Prof. Klaus Diepold</a></em>
      <br />
    <em>2018</em>
    <br />
      <a href="https://github.com/ussaema/Vector_Matrix_CapsuleGAN" target="_blank">Github Repo</a>
      <br />
This thesis introduces CapsGAN, a generative modeling framework that integrates capsule networks into the GAN discriminator to better preserve spatial relationships, evaluates different CapsGAN architectures and routing algorithms, and demonstrates that dynamic routing improves training stability and sample quality compared to standard GAN approaches.
</td>
  </tr>
</table>

<table class="responsive" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">  
  <tr>  
<td width="40%">
    <div class="one">
    <img src="/images/projects/ct.png" width="100%" class="img" />
</div>
  </td>
    <td valign="top" width="75%">
      <papertitle>
        <strong>
          <a href="https://github.com/ussaema/CT_Image_Reconstruction" target="_blank">
            Computed Tomography Image Reconstruction
          </a>
        </strong>
      </papertitle>
      <em>Project in the scope of Ferienakademie by TUM, University of Erlangen and University of Stuttgart</em>
    <br />
    <em>2018</em>
      <br />
      <a href="https://github.com/ussaema/CT_Image_Reconstruction" target="_blank">Github Repo</a>
      <br />
In this project, we develop a deep learning–based learnable filter to enhance the CT image reconstruction pipeline for improved image quality and reconstruction accuracy.
</td>
  </tr>
</table>

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&amp;w=400&amp;t=tt&amp;d=rM7BoV2_o5IxNyY7EAufsftBDgwOhxdU0h5gt6JOQ5o&amp;co=ffffff&amp;cmo=79c4d3&amp;cmn=3a90cc&amp;ct=80b2c6"></script>



        

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        
<!-- Support for MatJax -->
<script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>

<!-- Support for Plotly -->
<script defer src='https://cdnjs.cloudflare.com/ajax/libs/plotly.js/3.0.1/plotly.min.js'></script>

<!-- Support for Mermaid -->
<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
    mermaid.initialize({startOnLoad:true, theme:'default'});
    await mermaid.run({querySelector:'code.language-mermaid'});
</script>

<!-- end custom footer snippets -->

        


<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="http://github.com/ussaema"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
  </ul>
</div>


<div class="page__footer-copyright">
  &copy; 2025 Oussema Dhaouadi, Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.<br />
  Site last updated 2025-09-21
</div>

      </footer>
    </div>

    <script type="module" src="http://localhost:4000/assets/js/main.min.js"></script>








  </body>
</html>

